{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming of Data Analysis Project 1\n",
    "\n",
    "**Francesco Troja**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project 1\n",
    "\n",
    ">Create a data set by simulating a real-world phenomenon of your choosing. Then rather than collect data related to the phenomenon, you should model and synthesise such data using Python.We suggest you use the numpy.random package for this purpose. Specifically, in this project you should:\n",
    ">- Choose a real-world phenomenon that can be measured and for which you could collect at least one-hundred data points across at least four different variables.\n",
    ">- Investigate the types of variables involved, their likely distributions, and their relationships with each other.\n",
    ">- Synthesise/simulate a data set as closely matching their properties as possible.\n",
    ">- Detail your research and implement the simulation in a Jupyter notebook â€“ the data set itself can simply be displayed in an output cell within the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute this project, several Python libraries have been utilized. These libraries were chosen for their specific functionalities and capabilities, tailored to the requirements of the project:\n",
    "1. `padas`: The library's powerful data structures, including DataFrames and Series, allowed for efficient organization and structuring of data, making it easy to perform various data operations, such as filtering, grouping, and aggregating.Pandas offered a wide range of functions for data cleaning and preparation, making it ideal for addressing real-world data challenges[1].\n",
    "2. `matplotlib.pyplot`: It is a widely used library for data visualization in Python. It provides a flexible and comprehensive set of tools to create various types of plots and charts. Its versatility allows to create bar charts, line plots, scatter plots, histograms, and more, making it an essential tool for exploratory data analysis and presentation of findings[2].\n",
    "3. `numpy`: It is imported in this context for its extensive capabilities in numerical and statistical operations. Numpy provides a wide range of probability distributions, functions for generating random numbers following these distributions, and tools for statistical calculations[3]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided dataset offers an extensive and detailed record of property house prices in Ireland, spanning the period from 2010 to 2023. It plays a significant role in capturing essential information about the real estate market in Ireland. This dataset provides valuable insights into the dynamics of property prices, trends, and fluctuations over a thirteen-year period. The housing market is a crucial element of a country's economy, and property prices reflect not only home values but also broader economic conditions, as well as the forces of supply and demand at play.\n",
    "\n",
    "The dataset includes a wide range of variables, including the date of sale, property location, property type, and the sale price. These variables offer a rich source of information for analysis, allowing for the examination of various aspects of the property market, such as regional variations, property types, and the impact of economic events on house prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used was was discovered on the [Kaggle](https://www.kaggle.com/datasets/raphaelmapp/ireland-house-prices-2010-to-2023/code) website.\n",
    "\n",
    "In Python, working with CSV files often involves using the `read_csv()` function from the Pandas library. This function acts as a crucial tool, facilitating the smooth import of CSV files into a Pandas DataFrame. The DataFrame represents the data in a structured format, enabling easy manipulation and analysis. The DataFrame format, offered by Pandas, facilitates straightforward data exploration, manipulation, and analysis. In order to import the csv file, the file path is passed as parameter. This file path specifies the location of the CSV file you want to import. The read_csv() function then reads the data from that file and converts it into a Pandas DataFrame[2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"Housing_Data_Jan2010_to_May2023_Cleaned.csv\")\n",
    "\n",
    "print (f'The dataset used is:\\n {df}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate the dataset's  structure and characteristics. Statistical analysis is a method for uncovering patterns and correlations in data. The goal is to provide a descriptive overview of the dataset and its variables. Let's have a look at the dataset's contents:\n",
    "\n",
    "- The Pandas `head()` method is used to return the top n (default is 5) rows from a dataset.\n",
    "- The Pandas `tail()` method is used to return the bottom n (default is 5) rows from a dataset[3].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the first 5 rows of the dataset:\\n\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the last 5 rows of the dataset:\\n\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the provided code, the selected dataset comprises 597,527 rows and 8 columns. Additionally, it's apparent from this initial analysis that the dataset contains missing values. The dimensionality of the dataset, can be confirmed using the Pandas function `shape` that when used it returns a tuple where the first element represents the number of rows (observations) and the second element indicates the number of columns (variables) in the dataset[4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The dimensions of the dataset are:\\n')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain further insights into the DataFrame, the `info()` function can be used. This function provides metadata about the DataFrame, including the column names, the count of non-null values in each column, and the data type for each column[5]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Find below the full summary of the Dataset:\\n')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis of the dataset reveals the following key findings:\n",
    "\n",
    "1. The dataset consists of 8 columns:\n",
    "\n",
    "    - Date\n",
    "    - Address\n",
    "    - County\n",
    "    - Price\n",
    "    - Full_market_price\n",
    "    - VAT_Exclusive\n",
    "    - Description_of_Property\n",
    "    - Property_Size_Description\n",
    "\n",
    "2. It is apparent that there are missing values present in some of the columns.\n",
    "3. The dataset is composed of a mix of data types. Specifically, there are 5 columns with object data types (string of Text or mixed numeric and non-numeric values) and 3 numerical columns(Floating point numbers), which likely contain numeric information[6].\n",
    "\n",
    "The analysis highlights an issue with the \"date\" column being stored as an object data type, which limits its utility for datetime operations. To resolve this, converting the \"date\" column to the datetime64 data type is necessary. The `to_datetime()` function, a part of Pandas, serves this purpose. By using this function, the \"date\" column can be transformed into a format that enables effective datetime operations on the dataset[7]. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert Date into datatimes type\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"],dayfirst=True, errors='coerce')\n",
    "df[\"Date\"].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed in the previous examination using the tail() function, it became apparent that there are entire rows containing null values in the dataset.To determine the precise count of rows with null values, `isnull()` function can be used. The function identifies and flags the presence of missing or null values in the dataset. This understanding is essential before proceeding with further data analysis and allows for informed decision-making regarding how to handle these null value rows[8]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking Date Variable as references\n",
    "df[df['Date'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon analysis, it's evident that the total number of rows containing null values amounts to 873. Given that these rows do not contain any valuable information, the next step involves removing them from the dataset. The function `drop()` is used for this purpose, which eliminates rows based on their index values. Typically, the index value represents a 0-based integer value assigned to each row. By specifying the row index, it can be deleted from the dataset[9]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using inplace to change the dataset\n",
    "df.drop(df.index[596655:597528], inplace= True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis will now shift to examining the missing values within the variables of the dataset. To determine of many missing values exist for each variable the `sum()` function can be chained on the `isnull()`[10]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The missing values are:\\n')\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this analysis reveals that out of the 8 variables in the dataset, 5 of them contain missing values. Specifically, the \"Date\" variable has 1 missing value, while the \"Full_Market_Price,\" \"VAT_Exclusive,\" and \"Description_of_Property\" variables each have 11 missing values. Indeed, the approach to handling missing values depends on the specific analysis or task at hand. Identifying and understanding the nature of missing data is a critical aspect of data preparation. Depending on the goals of the analysis, various actions can be taken with missing values, including imputation (filling in missing values with estimated or calculated data) or employing data cleaning techniques to ensure the dataset's quality and suitability for the intended analysis[11]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the process of analyzing a dataset, a crucial initial step involves determining the type of variable associated with each attribute. One fundamental property of variables is their level or scale of measurement, which dictates the permissible arithmetic operations and, consequently, specifies the applicable statistical tests. In statistics, there are four primary levels of measurement: **nominal**, **ordinal**, **interval**, and **ratio**. These levels are hierarchical, with each level possessing all the characteristics of the previous levels, and some additional features[12].\n",
    "\n",
    "- Nominal Scale: This is the lowest level of measurement, indicating that variables possess distinct values, but no meaningful order can be established among them. When there are only two categories, such as gender, it is referred to as dichotomous or binary.\n",
    "- Ordinal Scale: Positioned one level higher, the ordinal scale encompasses nominal information but allows for the establishment of a ranking. However, the distances between values are not interpretable, making it impossible to quantify the absolute distance between two values[13].\n",
    "\n",
    "Variables with a nominal or ordinal scale are often termed categorical variables while Variables with ordinal,interval and Ratio scale are Continuous Variable[13].\n",
    "\n",
    "- Ordinal variables, categorize information with a clear sense of order or ranking. However, it's important to emphasize that the intervals or gaps between these categories are not uniform or quantifiable. For example, consider customer satisfaction ratings such as \"poor,\" \"fair,\" \"good,\" and \"excellent.\" While these categories can be ranked, the differences between them are not consistent and cannot be precisely measured. Ordinal scales allow for the establishment of a ranking, indicating higher or lower positions, but they do not provide a basis for making detailed numerical comparisons[14].\n",
    "- Interval Variables: This category permits the application of a wide array of statistical measures. However, it's essential to note that these measures cannot assume the existence of a 'true' zero point. On an interval scale, the zero point is a matter of convention rather than an absolute marker. For instance, Centigrade and Fahrenheit temperature scales both exhibit equal intervals of temperature defined by considering equal volumes of expansion. Yet, each scale establishes an arbitrary zero point, and numerical values from one scale can be translated into equivalent values on the other using a specific mathematical equation. The critical idea is that interval variables maintain their properties regardless of the choice of the zero point, as long as consistent transformations are applied[15].\n",
    "- Ratio Variables: Representing the highest level of precision among all scales, ratio data is a subset of quantitative data. Unlike interval data, ratio data possesses a distinctive attribute: the presence of a \"true zero.\" A zero measurement on a ratio scale is absolute, signifying that ratio data can never be negative. This characteristic enables the full range of mathematical operations, including addition, subtraction, multiplication, and division, during statistical analyses[16].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical data, a common summary measure is the count of observations for a specific category or percentage that each category contributes to the entire dataset. To visually represent this information, a frequency table can be utilized, often accompanied by a bar chart or pie chart. A frequency table displays the occurrence of each unique value within a column, providing both tabular and graphical representations[17].\n",
    "\n",
    "To identify the distinct values of categorical variables within the dataset, the Pandas `unique()` function can be employed. This function returns an array containing the unique values found in a specified column. For instance[18]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list that includes all the categorical variables\n",
    "cat_var = ['Address', 'County', 'Description_of_Property', 'Property_Size_Description', ]\n",
    "\n",
    "for variable in cat_var:\n",
    "    unique_value = df[variable].unique()\n",
    "    print(f'\\nUnique {variable} in the dataset:\\n', unique_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above reveals an issue where the \"Description_of_Property\" and \"Property_Size_Description\" attributes contain gibberish text. Further analysis, facilitated by the English-Gaelic translator ([focloir](https://www.focloir.ie/en/dictionary/ei/dwelling+house)), indicates that this gibberish text corresponds to the Gaelic version of the English text. To enhance the clarity of the dataset for analysis, it is preferable to remove the Gaelic version and replace it with the English equivalent. The `replace()` function will be employed for this purpose[19]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of substrings as key-value pairs\n",
    "char_to_replace = {\n",
    "    \"n?os l? n? 38 m?adar cearnach\": \"less than 38 sq metres\",\n",
    "    \"Teach/?ras?n C?naithe Nua\": \"Second-Hand Dwelling house /Apartment\",\n",
    "    \"Teach/ï¿½rasï¿½n Cï¿½naithe Nua\": \"Second-Hand Dwelling house /Apartment\",\n",
    "    \"Teach/ï¿½rasï¿½n Cï¿½naithe Athï¿½imhe\": \"New Dwelling house /Apartment\",\n",
    "    \"nï¿½os mï¿½ nï¿½ nï¿½ cothrom le 38 mï¿½adar cearnach agus nï¿½os lï¿½ nï¿½ 125 mï¿½adar cearnach\": 'greater than or equal to 38 sq metres and less than 125 sq metres'\n",
    "}\n",
    "\n",
    "for old_text, new_text in char_to_replace.items():\n",
    "    df['Property_Size_Description'] = df['Property_Size_Description'].replace(old_text, new_text)\n",
    "    df['Description_of_Property'] = df['Description_of_Property'].replace(old_text, new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's move on with the analysis. To obtain the count of unique values for a categorical variable, the function [`value_counts()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html) can be used. This function provides a series containing counts of unique values in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_var = ['County', 'Address', 'Description_of_Property', 'Property_Size_Description']\n",
    "\n",
    "for variable in cat_var:\n",
    "    count = df[variable].value_counts()\n",
    "    print (f'\\nValue counts for column:\\n {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's proceed to visualize the distribution of categorical data by plotting a pie chart. This graphical representation will illustrate the percentage contribution of each category to the overall dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_value = df.County.value_counts()\n",
    "desc_prop_value = df.Description_of_Property.value_counts()\n",
    "size_prop_value = df.Property_Size_Description.value_counts()\n",
    "\n",
    "fig = plt.figure(figsize=(50, 25)) # create a figure with a 50 width, 25 length\n",
    "\n",
    "ax1 = plt.subplot(131) #subplot with 1 row, 3 columns the 1st one\n",
    "ax2 = plt.subplot(132) #subplot with 1 row, 3 columns the 2nd one\n",
    "ax3 = plt.subplot(133) #subplot with 1 row, 3 columns the 3rd one\n",
    "\n",
    "county_value.plot(kind='pie', x=county_value, y = county_value.index, autopct='%1.1f%%', ax= ax1)\n",
    "desc_prop_value.plot(kind='pie', x=desc_prop_value, y = desc_prop_value.index, autopct='%1.1f%%', ax= ax2)\n",
    "size_prop_value.plot(kind='pie', x=size_prop_value, y = size_prop_value.index, autopct='%1.1f%%', ax= ax3)\n",
    "ax1.set_title('Counties',  fontsize=40)\n",
    "ax2.set_title('Property Description',  fontsize=40)\n",
    "ax3.set_title('Property size', fontsize=40)\n",
    "plt.savefig(\"percentage of categorical variable\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1]: Chugh v., (2023). \"*Python pandas tutorial: The ultimate guide for beginners*\".[Datacamp](https://www.datacamp.com/tutorial/pandas)\n",
    "\n",
    "[2]: matplotlib, (n.d.). \"*matplotlib.pyplot*\". [matplotlib](https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html)\n",
    "\n",
    "[2]: Analyseup, (n.d.). \"*Importing Data with Pandas*\". [Analyseup](https://www.analyseup.com/learn-python-for-data-science/python-pandas-importing-data.html)\n",
    "\n",
    "[3]: Shazra H., (2023). \"*head () and tail () Functions Explained with Examples and Codes*\". [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2023/07/head-and-tail-functions/)\n",
    "\n",
    "[4]: Pandas, (n.d.). \"*pandas.DataFrame.shape*\".[Pandas](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html)\n",
    "\n",
    "[5]: Rajan S., (2023). \"*Python | Pandas dataframe.info()*\". [geeksforgeeks](https://www.geeksforgeeks.org/python-pandas-dataframe-info/)\n",
    "\n",
    "[6]: Moffitt C., (2018). \"*Overview of Pandas Data Types*\". [Practical Business Python](https://pbpython.com/pandas_dtypes.html#:~:text=An%20object%20is%20a%20string,df)\n",
    "\n",
    "[7]: stackoverflow, (2014). \"*Convert Pandas Column to DateTime*\". [stackoverflow](https://stackoverflow.com/questions/26763344/convert-pandas-column-to-datetime)\n",
    "\n",
    "[8]: Data to Fish, (2021). \"*Select all Rows with NaN Values in Pandas DataFrame*\". [Data to Fish](https://datatofish.com/rows-with-nan-pandas-dataframe/)\n",
    "\n",
    "[9]: Lynn S., (n.d.). \"*Delete Rows & Columns in DataFrames Quickly using Pandas Drop*\".[Shane Lynn](https://www.shanelynn.ie/pandas-drop-delete-dataframe-rows-columns/)\n",
    "\n",
    "[10]: Welck Aj, (n.d.). \"*How to Check If Any Value is NaN in a Pandas DataFrame*\". [Chartio](https://chartio.com/resources/tutorials/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe/)\n",
    "\n",
    "[11]:Shashank S., (2023). \"*Defining, Analysing, and Implementing Imputation Techniques*\". [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/06/defining-analysing-and-implementing-imputation-techniques/)\n",
    "\n",
    "[12]: Kirch, Wilhelm, ed. (2008). \"*Level of Measurement*\". Encyclopedia of Public Health. [Springer Link](https://link.springer.com/referenceworkentry/10.1007/978-1-4020-5614-7_1971)\n",
    "\n",
    "[13]: DATAtab Team (2023). \"*Level of measurement*\". [DATAtab: Online Statistics Calculator](https://datatab.net/tutorial/level-of-measurement)\n",
    "\n",
    "[14]: GraphPad, (n.d.). \"*What is the difference between ordinal, interval and ratio variables? Why should I care?*\". [GraphPad](https://www.graphpad.com/support/faq/what-is-the-difference-between-ordinal-interval-and-ratio-variables-why-should-i-care/)\n",
    "\n",
    "[15]: Stevens S.S., (1946). \"*On the Theory of Scales of Measurement*\". Science, Volum. 103, No. 2684\n",
    "\n",
    "[16]: Bhat A., (n.d.). \"*Levels of Measurement: Nominal, Ordinal, Interval & Ratio*\". [QuestionPro](https://www.questionpro.com/blog/nominal-ordinal-interval-ratio/)\n",
    "\n",
    "[17]: Statgraphics19, (n.d.). \"*Categorical Data Analysis*\". [Statgraphics19](https://www.statgraphics.com/categorical-data-analysis#:~:text=The%20Frequency%20Tables%20procedure%20analyzes,a%20set%20of%20multinomial%20probabilities.)\n",
    "\n",
    "[18]: Ebner J., (2020). \"*How to Use Pandas Unique to Get Unique Values*\". [Sharp Sight](https://www.sharpsightlabs.com/blog/pandas-unique/)\n",
    "\n",
    "[19]: yuktijain, (n.d.). \"*How to replace multiple substrings of a string in Python?*\". [Study Tonight](https://www.studytonight.com/python-howtos/how-to-replace-multiple-substrings-of-a-string-in-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stackoverflow, (2020). \"*Python how to fix year out of range error*\".[stackoverflow](https://stackoverflow.com/questions/62130640/python-how-to-fix-year-out-of-range-error)\n",
    "- Stackoverflow, (2017). \"*Understanding inplace=True in pandas*\". [Stackoverflow](https://stackoverflow.com/questions/43893457/understanding-inplace-true-in-pandas)\n",
    "- matplotlib, (n.d.). \"*matplotlib.pyplot.pie*\". [matplotlib](https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.pie.html)\n",
    "- Amipara K., (2017). \"*Better visualization of Pie charts by MatPlotLib*\".[Medium](https://medium.com/@kvnamipara/a-better-visualisation-of-pie-charts-by-matplotlib-935b7667d77f)\n",
    "- Stackoverflow, (2020). \"*How to plot 3 plots simultaneously in one plot?*\". [Stackoverflow](https://stackoverflow.com/questions/61547691/how-to-plot-3-plots-simultaneously-in-one-plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
